线代主要处理连续数学（而非离散数学），所以很多计算机科学家接触较少，
但要理解机器学习算法，掌握线代很有必要。

- 标量  整数、实数等
- 向量  一列有序的标量
  （一维数组；可看作是空间中的点，每个元素是不同坐标轴上的坐标;
    后文中列向量和向量完全等价，这里没有行向量这种东西）
- 矩阵  若干个有序横排的同维向量 （二维数组）
- 张量  本书中指高维数组

矩阵（不限定为方阵）的主对角线  由纵横坐标相同的元素构成  (M_{1,1}, M_{2,2}, ..., M_{i,i})
若M为m行n列，则i=min(m,n)

矩阵的转置  将矩阵的第i行变换为新矩阵的第i列

- 矩阵相加（同维矩阵）
- 矩阵数乘
本书中允许矩阵和向量相加，矩阵A加向量b也就是把向量b加到A的每一列上去（numpy的广播）。


* 矩阵乘法
C=AB, A矩阵的列数必须等于B的行数
C的第i行j列的元素，为A的第i行乘以B的第j列

顺及，有种两个矩阵中对应元素分别相乘的操作，称为元素对应乘积。

交换率、结合率、分配率，矩阵乘法只服从后两条（显然不可交换，维数都未必满足）。

(AB)^{T}=B^{T}A^{T}

向量x,y的点积定义为x^{T}y，结果为一标量。

单位矩阵为主对角线元素为1，其他元素为0的方阵，记为I_{n}。

定义逆矩阵  A^{-1}A=I

线性方程组可以表示为Ax=b，A为已知矩阵，b为已知向量，x为未知向量。
利用逆矩阵，易得x=A^{-1}b

注意，在数值求解上述线性方程组的时候，通常不直接求A^{-1}，这是因为
逆矩阵在数字计算机上只能表现出有限精度，而有效利用向量b的算法可以得到更精确的x。


* 线性相关&生成子空间
线性方程组可能没有解，也可能有1个或者无穷多个解（但不会是其他情况，比如2个解，或者5个解）

把Ax=b展开为标量运算，可以看到，式子左边的计算过程可以写为
x_{1}A_{:,1}+x_{2}A{:,2}+...
也就是把A的每一个向量A_{:,i}乘以x的对应元素x_{i}，并求和

这种把各向量先分别数乘，最后求和的操作，称为线性组合。
一组向量线性组合所能得到的所有向量的集合，称为这组向量的生成子空间（span）。

确定Ax=b是否有解，就相当于确定b是否在A矩阵向量的生成子空间内。

下面我们集中讨论，对于任意的m维实向量b，Ax=b是否总是有解的问题。
显然，这个问题的答案应取决于A的性质和b的维数。
具体的，为了保证该方程总是有解，A的向量应能表出（张成）整个m维空间。

能表出整个m维空间，意味着A至少有m列（否则A的向量所表空间维数必小于m。
比如只给两个列向量，它们最多张成一个平面，而不可能张成整个三维空间。
这两个给定向量的长度甚至都不重要，这个长度只能决定所张的二维平面是在怎样的空间里）。

A有m列是必要条件，但并不充分——不能保证方程对每一个点都有解
（比如某一列能被其他m-1列线性表出的情况——线性相关的情况）。

所以，充分条件是A恰好包含m个线性无关向量（多于m个线性无关向量的情况？
因为这里每个向量长度为m，m个线性无关向量已能张成整个m维空间，不会出现多于m个线性无关的向量）。

如果希望方程只有一个解，则需矩阵A最多包含m个向量，否则方程会有多于一组解（
取前几列可以凑出b，取后几列也可以凑出b，传说中的降维打击）。

因此，要保证方程有唯一一组解，A需为一个方阵，且其维度也为m，且列向量线性无关(非奇异)。


* 范数
范数norm用于衡量向量的大小（距离原点的距离）。

2范数||x||(欧式距离)在ML中用的相当广泛。
2范数的平方在计算上甚至更加方便，因为它对x的各个元素的导数，只取决于对应元素
（作为对比，计算2范数对各元素的导数时，需处理整个向量）。

然而，平方2范数也有缺点，就是在原点附近增长太慢。
（这种情况下，1范数可能更加合适）。

最大范数/无穷范数定义为向量中最大幅值元素的绝对值。


* 特殊矩阵和向量
对角矩阵只在主对角线有非零值，其他元素全是零（不要求为方阵）。
对角阵求逆（和矩阵乘积）非常容易。

对称矩阵是转置与自身相等的方阵。
当某些不依赖参数顺序的双参数函数生成元素时，对称矩阵经常出现。
如距离度量矩阵、协方差矩阵等。

单位向量||x||=1

正交矩阵：方阵的每个列向量都是单位向量，且两两正交（正交：点积为0）。
因此有，A^{T}A=AA^{T}=I。这意味着A^{-1}=A^{T}，求逆代价很小。
（注意，方阵的左逆和右逆总是相等。）

注意，对于向量两两正交，但不是单位向量的方阵，没有专有术语。。


* 特征分解
可以通过分解矩阵，来发现矩阵表现为数组时不明显的性质。
/特征分解/是应用最广的方阵分解之一，它将方阵分解为特征向量和特征值。
方阵A的特征向量，是与A相乘后相当于对该向量进行缩放的非零向量，即Av=kv
其中，v是向量，k是标量。

注意，根据前面从线性方程组获得的理解，Av实际是A的向量组根据v做的一种线性表出。
对于任意向量v，Av将得到另一个向量。因此，也可以视A为一种变换。
比如，将A的向量看作一组基（坐标轴），那么Av就是该向量在这组基上的表示（坐标变换）。
这里的特征向量是，在该坐标变化下，方向不变的向量。

通常只考虑右特征向量，且只考虑单位特征向量。

不是每个矩阵都可以分解为特征值和特征向量。
幸运的是，本书通常只分解一类有简单分解的矩阵——实对称矩阵，它们可以分解为A=QWQ^{T}
其中Q是A的特征向量组成的正交矩阵，W是特征值（在主对角线上）组成的对角矩阵。

当且仅当所有特征值都唯一时，矩阵的特征分解唯一。

- 矩阵是奇异的，当且仅当矩阵有0特征值。
- 所有特征值都为正（负）的矩阵，称为正定（负定）矩阵。
- 所有特征值都非负（非正）的矩阵，称为半正定（半负定）矩阵。

实对称矩阵的特征分解可以用于优化二次方程f(x)=x^{T}Ax，其中x的范数限制为1。
当x等于A的某个特征向量时，f将返回对应的特征值。
在范数限制条件下，函数f的最大（最小）值是最大（最小）特征向量。

半正定矩阵受关注，是因为对于人员的向量x，有x^{T}Ax>=0。
对于正定矩阵，还保证x^{T}Ax=0 => x=0。


* 奇异值分解
通过奇异值分解，可以获得与特征分解同类型的信息。
但奇异值分解应用更广，每个实矩阵都有一个奇异值分解。

A=UDV^{T}, 其中U、V均为正交矩阵，矩阵D为对角阵。
假设A为mxn维，则U、D、V分别为mxm, mxn, nxn维。


* Moore-Penrose伪逆
A不是方阵时，Ax=b解的性质不定，且不能尝试利用逆矩阵求解。

定义伪逆为A^{+}=VD^{+}U^{T}，其中U、D、V是矩阵A的奇异分解得到的。
对角阵D的伪逆D^{+}为其非零元素取倒数，然后做矩阵转置得到的。

当矩阵A的列数多于行数时，伪逆求解（x=A^{+}y）是所有可行解中范数最小的一个。
当矩阵A的列数少于行数时，伪逆得到的x（=A^{+}y）可以使Ax与y的欧式距离最小。


* 迹
矩阵对角元素之和


* 行列式
方阵A的行列式det(A)等于A的特征值之积。
行列式的绝对值可用于衡量矩阵参与乘法之后（坐标变换后），空间扩大或缩小了多少。

若行列式为0，则空间至少沿某一维完全收缩了；若行列式为1，则该变换保持空间体积不变。


* PCA
按我之前的理解，PCA主要是通过计算协方差矩阵的特征系统，计算并选择出最具代表性的少数几列属性。
这本书显然是从另一个角度理解的（给定限定条件下的最小误差重构？），篇幅较长，想必很有意思，稍后再看。
