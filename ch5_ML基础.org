ML本质上属于应用统计学，更多地关注如何利用计算机统计地估计复杂函数。
ML不太关注为这些函数提供置信区间。


* 学习算法

**  任务
这里，任务是指如何处理样本。
ML可以解决很多类型的任务，
- 分类
- 回归
- 结构化 （转录、翻译、语法分析、图像分割等）


**  性能
我们更关注ML算法在未观测数据上的表现。
选择一个与系统理想表现相对应的性能度量通常是比较难的，这方面的设计应取决于应用。
还有一些情况，我们知道应该度量哪些指标，但实现起来不太现实。


**  经验
无监督学习试图学习出样本的概率分布，或者是该分布的一些有意思的性质；
监督学习观察输入的随机向量x及其标签y，然后试图从x预测y，通常是估计p(y|x)。

半监督学习处理只有部分样本有标签的情况。

有些ML算法并不是训练于一个固定的数据集上。例如，
强化学习算法会和环境交互，所以学习过程和它的训练过程会有反馈回路。
（本书不讨论这方面的内容——too bad）。。

数据集常表示为设计矩阵（宽表）。

有时候，标签会比较复杂，如训练语音模型转录整个句子时，每个句子的标签都是一个单词序列。


** 示例：线性回归
对每个输入的测试记录，对其各个特征做线性组合（各维分别乘以权重，然后求和），以估计其目标值（标签）。
这里，用均方误差度量模型的性能。

通过把误差函数对权重求偏导，求得临界点（极小值）处的权重向量。
书中把通过测试向量集及其标签集，求取权重的等式称为正规方程。

具体的，假设输入向量为x \in *R*^{n}，标签为y \in *R*，算法预测值为\hat{y}。
因为这里的算法是线性的，我们有
\hat{y}=x^{T}w，其中w \in *R*^{n}为算法的权重参量。

预测误差用均方误差度量，即E_{train}=\sum_{i=1..m}(\hat{y}_{i}-y_{i})^{2} /m，
其中，m为训练集中样本的个数。

模型训练时，我们的目标是使E_{train}尽可能地小。
上面式子可以看出，E是正定的，直接求其临界点即可:
- ∇_{w}E_{train} = 0  # 求E对w的临界点，要求的是w
- ∇_{w}\sum_{i=1..m}(\hat{y}_{i}-y_{i})^{2} /m = 0  # 代入
- ∇_{w}\sum_{i=1..m}(\hat{y}_{i}-y_{i})^{2} = 0  # 消除因子
- ∇_{w}(\hat{y}-y)^{T}(\hat{y}-y) = 0  # 写为向量形式
- ∇_{w}(Xw-y)^{T}(Xw-y) = 0  # 代入
- ∇_{w}((Xw)^{T}-y^{T})(Xw-y) = 0
- ∇_{w}((Xw)^{T}Xw-2y^{T}Xw+y^{T}y) = 0 # 注意，括号内为标量
- 2X^{T}Xw-2y^{T}X = 0
- w=(X^{T}X)^{-1}(X^{T}y)

关于偏置


*  泛化能力
与优化不同的是，ML中我们希望泛化误差尽可能地小（测试集等尚未观测到的数据集上的误差）。

当我们只用训练集训练模型时，如何才能影响到测试集上的性能？独立同分布假设。
（两个数据集中的样本是互相独立的，并且两个数据集同分布）。

决定ML算法效果好坏的依据：
- 训练误差小  （否则，欠拟合）
- 训练误差与测试误差的差值小  （否则，过拟合）


** 容量
容量是ML模型拟合各种函数的能力。
容量太低的模型容易导致欠拟合。
容量太高的模型又容易产生过拟合。

一种控制模型容量的方法，是选择假设空间，也就是选择ML算法的可使用的函数集。
比如引入x^{2}作为线性拟合的可用函数(注，此时模型的输出仍是w的线性函数，仍可用正规方程求解)。

奥卡姆剃刀


** VC维度
量化模型容量的方法。定义为二分类器能够将训练样本分类的最大分类数。(原来的翻译坑真不少。。)

统计学习理论中最重要的结论阐述了，训练误差与测试误差间差值的上界
随模型容量的增长而增长，随训练样本的增多而下降。

非参数模型，如最近邻回归

** 没有银弹
没有一个ML算法，在所有可能的数据分布上，都比其他的ML算法好。
（在考虑所有的数据分布后，所有的ML算法都一样好/一样糟。）

** 正则化
正则化是修改ML算法，使其泛化误差降低（而非训练误差）。
正则化是ML的中心议题之一，其重要性之高只有优化能与之相提并论。
正则化的思路之一是，给模型的目标函数/代价函数添加代表模型复杂程度的惩罚项（正则化项）。
DL的理念是，大量任务或许都可以使用非常通用的正则化项有效地解决。

