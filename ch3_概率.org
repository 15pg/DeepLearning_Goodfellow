这一章主要讨论概率论。它不仅提供了描述不确定性的方法，也提供了用于导出新的不确定性声明的公理。
它是众多科学学科和工程学科的基本工具。
在AI领域，概率论主要有两种用途，首先，它告诉我们AI系统如何推理，其次它可以从理论上分析AI系统的行为。

不确定性的来源：
- 被建模系统的内秉随机性
- 不完全观测
- 不完全建模

很多情况下，使用简单而不确定的规则，要比使用复杂而确定的规则更为实用。
比如：“大多数鸟都会飞”这个简单规则，要比“除了幼鸟、病鸟，以及鸵鸟...等不会飞的鸟之外，鸟儿都会飞”这个描述
可用范围更广，更容易沟通和维护。另外，后者也更加脆弱，容易失效。

- 概率论最初是为了分析事件发生的频率。这类事件往往是可以重复的。
-- 这样，当我们说一个事件发生概率为p时，我们指的是反复重复实验时，有p的概率导致这样的结果。

- 而另一种情况下，我们用概率表示可信度。这类事件往往不可重复。
-- 比如，当医生说一个病人患流感概率为40%时，就是这种情况（1:肯定患流感, 0:肯定没患流感）。

前者称为频率派概率，后者称为贝叶斯概率。
要满足不确定性常识推理所应具有的性质，唯一一种方法就是将这两种概率视为等同。

逻辑可以在给定命题是真是假的前提下，判断另一些命题是真是假。
概率论则可以根据给定命题的似然，计算其他命题的似然。
从这个意义上，概率论可以看作是逻辑在处理不确定性问题上的扩展。


* 随机变量
随机变量是可以随机地取不同值的变量。
就其本身而言，一个随机变量只是对可能状态的描述；它必须伴随一个概率分布来指定取值为每个状态的可能性。
随机变量可以是离散的(有限个或可列个状态)和连续的(状态不可列)。


* 概率分布
概率分布用于描述随机变量或一组随机变量在每一个可能取到状态的可能性的大小。
概率分布的描述方式取决于随机变量是连续的还是离散的。

- 离散随机变量和概率质量函数PMF
读者通常需根据随机变量来推断所使用的PMF，而不是根据PMF的函数名。
PMF将随机变量能取到的每个状态映射到随机变量取得该状态的概率。
PMF写为P(x=a)或简写为P(a)，其中x为随机变量，a为x可能取到的一个状态。
有时也先定义一个随机变量，并使用波浪线表示它服从的分布： x~P(x)

PMF可同时作用于多个随机变量，这种情况称为联合概率分布。
P(x=a, y=b)表示x=a和y=b这两个时间同时发生的概率，简写为P(a, b)

PMF需同时满足
--- P的定义域需为x所有可能的状态
--- 0 <= P(a) <= 1
--- 归一化条件

- 连续随机变量和概率密度函数PDF
PDF函数p(x)不直接给出特定状态的概率，相对的，它给出了落在面积为dx的无限小区域内的概率：p(x)dx。
可以通过对PDF求积分来获得点集的真实概率质量。

与PMF不同的是，对PDF并不要求p(a)<=1，（但仍要求p(a)>=0和归一化条件）。

- 边缘概率
给定一组随机变量的联合分布后，其每一个随机变量子集的概率分布就是边缘概率分布。


* 条件概率、贝叶斯规则
条件概率是，某个事件（如y=b）在其他事件（如x=a）发生时出现的概率，记为P(y=b|x=a)。

链式法则 P(y=b, x=a) = P(y=b|x=a)*P(x=a)

贝叶斯规则


* 独立性
独立 P(y=b, x=a) = P(y=b)*P(x=a)

条件独立 P(y=b, x=a | z=c) = P(y=b|z=c)*P(x=a|z=c)


* 期望、方差、协方差

期望求取的是平均值，它是线性的。

方差（variance）衡量的是差异性。方差的平方根被称为标准差（standard deviation）。

协方差（covariance）在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度。

其他的衡量指标如相关系数（correlation）将每个变量的贡献归一化，为了只衡量变量的相关性而不受各个变量尺度大小的影响。

独立性比零协方差的要求更强，因为独立性还排除了非线性的关系。

协方差矩阵的对角元是方差。


* 常用概率分布
- 白努力分布  单个二值随机变量的分布
- 多努力分布  单个多值随机变量的分布(有限个取值状态)
- 高斯分布
--- 很多独立随机变量的和近似服从高斯分布。
--- 具有相同方差的所有可能分布中，高斯分布在实数上具有最大不确定性。
--- 精度矩阵（协方差矩阵的逆）

- 指数分布与Laplace分布

- Dirac分布与经验分布

- 分布的混合
--- 对简单的概率分布进行组合，生成更丰富的分布
--- 任何平滑概率密度都可以用具有足够多组件的高斯混合模型以任意精度逼近。


* 常用函数的有用性质
- logistic sigmoid func: e^{x}/(1+e^{x})
- plus func: max(0, x)
- soft plus func: log(1+e^{x})
- logit func (logistic sigmoid func的逆函数): log(x/(1-x))

对soft plus函数求微分，可以得到logistic sigmoid函数。
另外还有一些有趣的性质，但这里公式太难打了。


* 测度
本书只对相对简单的集合进行积分，因此不太需要动用测度论。
- 零测度  这种集合不占体积
- 几乎处处

* 信息论
- 香浓熵（比香农好听多了哦）
- KL散度 衡量两个单独的概率分布P、Q的差异
--- KL散度非负；且当且仅当P、Q几乎处处相同时，取到0值。
- 交叉熵

* 图模型
按照前例，暂时跳过。
