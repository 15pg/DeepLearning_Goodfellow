机器学习需要用到大量的数值计算，这通常用于迭代求解。
常见的问题包括优化问题，和线性方程组求解。

* 溢出
在数字计算机上处理连续数学的根本困难是，需要用有限的若干位表示无限多的实数。
这意味着，数字计算机在表示实数时，不可避免地要引入近似误差。

- 下溢  接近0的数字被舍入为0（从而在零除或求log时会出现问题）
- 上溢  大量级的数被近似为无穷大

关于softmax和log softmax函数的溢出处理.
softmax(x)的第i项为 e^{x_{i}}/\sum_{j=1..n}e^{x_{j}}

- 深度学习库的底层开发者应牢记数值问题，而一般读者可以简单地依赖底层库。
- Theano软件包可以自动检测并稳定深度学习库中许多常见的数值不稳定表达式。


* 病态条件
存在有些函数，当输入被轻微扰动时，函数结果会迅速改变。
对于科学计算而言，这类函数可能引发一些问题，因为舍入误差会导致函数结果的巨大变化。

考虑函数f(x)=A^{-1}x，当矩阵A具有特征分解时，其条件数定义为\max_{i,j}|\lambda_{i}|/|\lambda_{j}|
（原译注：与通常的条件数定义有所不同。）

当该数很大时，矩阵求逆对输入误差特别敏感。
即使乘以完全正确的逆阵，病态矩阵也会放大预先存在的误差。


* 基于梯度的优化
优化是指通过改变x以最小化（或最大化）函数f(x)的任务。大多数深度学习算法都涉及某种形式的优化。
通常用一个上标*表示最小化（或最大化）函数的x值：x^{*}=argmin f(x)。


** 梯度（一阶偏导）
导数对于最小化一个函数很有用，因为它告诉我们如何调整x能略微地减小f(x)。即梯度下降。

f'(x)=0的点为临界点；在临界点处，导数无法提供往哪个方向移动的信息。
临界点可能为极大点、极小点、或鞍点(如x^{3}在x=0点处的情况)

我们经常处理具有输入变量x为n维向量的情况；为了使最小化的概念有意义，函数f的输出必须是一维的。
独立对多个目标进行决策不是这里考虑的问题，相反，这里总是把多个目标综合成一个。

针对具有多维输入的函数，我们需要用到偏导数，也就是函数值相对单个自变量的变化。
梯度是包含所有偏导数的向量，它总是指向曲面上升最快的方向。
在多维情况下，临界点是梯度中所有元素都为0的点。

方向导数是函数在给定方向的斜率，它等于梯度在该方向上的投影。

在负梯度方向上移动自变量，可以最迅速地减小函数值，该方法因此被称为最速下降，或梯度下降。

梯度用\nabla (▽)表示，最速下降建议的新自变量点为x'=x-a▽_{x}f(x)，其中a>0为学习率。

学习率的普遍选择方法是选取一个小的常数。
或者，根据几个a，分别计算x'，并选择能产生最小目标函数值的a，这种方法被称为线搜索。

虽然梯度下降被限定在连续空间的优化问题，但其沿着当前最优方向移动一小步的思想，也可以推广到离散空间（爬山法）。


** 海森矩阵（Hessian，二阶）
二阶导数是对曲率的衡量，它提供了额外的信息用于改善优化问题的搜索求解。

当函数f具有多维输入时，二阶导数也有很多。可以将这些导数合并到一起，构成海森矩阵。
海森矩阵H(f)(x)定义为\partial^{2} f(x)/(\partial x_{i} \partial x_{j})，
它等价于梯度的亚克比矩阵。

微分算子在任意二阶偏导连续的点处可交换，也就是H_{i,j}=H_{j,i}，海森矩阵在这些点上对称。
在深度学习背景下，我们遇到的大多数函数的Hessian几乎处处都是对称的，可以分解为一组实特征值和一组特征向量的正交基。

在特定方向d上的二阶导数可以写成d^{T}Hd。
当d是H的一个特征向量时，这个方向的二阶导数就是对应的特征值。
对于其他的方向d，方向二阶导数是所有特征值的加权平均（权重在0到1之间）。
最大特征值对应最大的二阶导数，最小特征值对应最小二阶导数。

可以通过（方向）二阶导数预期一个梯度下降步骤的表现。
在当前点x^{(0)}处，做函数f(x)的近似二阶泰勒展开：
f(x)=f(x^{(0)})+(x-x^{(0)})^{T}g+0.5*(x-x^{(0)})^{T}H(x-x^{(0)})，
其中，g是梯度，H是f在x^{(0)}处的海森矩阵。

在x^{(0)}点做最速下降时，新的x点将是x^{(0)}-ag (其中，a>0是步长)。
将x=x^{(0)}-ag，代入上述二阶近似可得
f(x^{(0)}-ag)=f(x^{(0)})-ag^{T}g+0.5*a^{2}g^{T}Hg。
式中的三项分别是函数的原始值，斜率导致的修正和曲率导致的修正。

当最后一项太大时，梯度下降实际上可能向上移动，因此需要仔细地选择步长a。
容易看出，当最后一项g^{T}Hg为0或负时，a可以选大一些，只要在二阶泰勒展开的描述范围内即可。
而当最后一项为正时，步长a有明确的上限，具体的，
令后两项-ag^{T}g+0.5*a^{2}g^{T}Hg<=0，则有
-g^{T}g+0.5*ag^{T}Hg<=0,
0.5*ag^{T}Hg<=g^{T}g,
a<=2g^{T}g/(g^{T}Hg).
在最坏的情况下，g与H的特征向量对齐，则最优步长为2/\lambda_{max}
（注意：原书分子为1？！！）.

H的特征值能告诉我们临界点的类型。
当H所有特征值都为正时，H正定，该临界点为极小点。
H负定，该临界点为极大点。
当H所有特征值有正有负时，该临界点为鞍点。
H半正定时，该临界点为弱极小点（凹槽）。
H半负定时，该临界点为弱极大点（山脊）。

当H为条件数很大的病态矩阵时，步长很难选择。
此时可以使用牛顿法，一次（或少数几次）跳到函数最小点。
但只有附近的临界点是最小点时，牛顿法才适用。

牛顿法的具体思想是，首先用二次函数逼近f
f(x)=f(x^{(0)})+(x-x^{(0)})^{T}g+0.5*(x-x^{(0)})^{T}H(x-x^{(0)})（上面的泰勒展开），
然后求其临界点（梯度为0的点）：梯度为g+H(x-x^{(0)})，
令其为0, 可解得 x-x^{(0)}=-H^{-1}g，即
x^{*}=x^{(0)}-H^{-1}g （书中Eq.4-12式子是这么来的）

若f是正定二次函数，上式可一步求得x^{*}；
若f不是真正二次，但能在局部近似为正定二次，则需几次迭代以达到临界点。

梯度下降不会被吸引到鞍点。

梯度下降称为一阶优化算法（只利用梯度），牛顿法为二阶优化算法（利用梯度和曲率）。

深度学习背景下，函数f满足Lipschitz连续。
这个属性允许我们自己量化自己的假设——梯度下降等算法导致的输入微小变化将使输出只产生微小变化。
而且，Lipschitz连续条件是相当弱的约束，深度学习中很多的优化问题经小的修改就能变得Lipschitz。

一般说来，凸优化在深度学习背景下的重要性不是很高。

* 约束优化
暂时跳过，回头再看。

* 线性最小二乘
惯例跳过。
